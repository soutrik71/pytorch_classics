{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhWrySkr2C1R7s7fd7bpyk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soutrik71/pytorch_classics/blob/main/APTorch7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The focus of this notebook is to implement LSTM for sequence generation tasks\n",
        "1. Text Generation using LSTM Networks (Character-based RNN)\n",
        "2. Text Generation using PyTorch LSTM Networks (Character Embeddings)\n",
        "3. Sequence generation at word level using LSTM Networks (Word Embeddings)\n",
        "3. Application of pre-trained embedding models for the better representation of words"
      ],
      "metadata": {
        "id": "bg4GinT-Aulr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker\n",
        "!pip install torchview\n",
        "!pip install torcheval\n",
        "!pip install scikit-plot\n",
        "!pip install lime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFpjeyixqhUe",
        "outputId": "a1d1c1c9-aef1-4d4f-acc0-95a7f866bc6b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n",
            "Collecting torchview\n",
            "  Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: torchview\n",
            "Successfully installed torchview-0.2.6\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.10.0)\n",
            "Installing collected packages: torcheval\n",
            "Successfully installed torcheval-0.0.7\n",
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.11.4)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->scikit-plot) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n",
            "Installing collected packages: scikit-plot\n",
            "Successfully installed scikit-plot-0.3.7\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (24.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283835 sha256=44912a77afaea605921ddc61b1a9c558be7e1f3eeb70c9f0c00d7f368b4bba7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "ZCF0Br6zqheu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import re\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchsummary import summary\n",
        "from torchview import draw_graph\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torcheval.metrics import MulticlassAccuracy,BinaryAccuracy\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import scikitplot as skplt"
      ],
      "metadata": {
        "id": "0pcl4he8qhpQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 42) -> None:\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    print(f\"Random seed set as {seed}\")"
      ],
      "metadata": {
        "id": "oEYBAP9MqhsT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set manual seed since nn.Parameter are randomly initialzied\n",
        "set_seed(42)\n",
        "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "batch_size = 1024\n",
        "epochs = 10\n",
        "lr = 1e-4\n",
        "embedding = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_jHFiYvqhvW",
        "outputId": "6e404d15-9ff1-4a68-d95c-d10adcfe7bb9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set as 42\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic DataPrep\n",
        "\n",
        "In this section, we undertake data preparation for training the neural network using a character-based approach. Specifically, we adopt a fixed sequence length of 100 characters, where the network's task is to predict the subsequent character given this sequence. Employing an embeddings technique, we encode text data by assigning a unique real-valued vector to each character.\n",
        "\n",
        "The data preparation procedure is as follows:\n",
        "\n",
        "* Loading Text Examples and Creating Vocabulary: Iterate through all text examples to construct a vocabulary, mapping each character to a distinct integer index. This vocabulary\n",
        "facilitates character representation in a numerical format.\n",
        "\n",
        "* Organizing Data with a Sliding Window: Implement a sliding window mechanism to organize the data. For every text example, we slide a window of 100 characters. The first 100 characters serve as input features (X), while the 101st character becomes the target value (Y). This process continues by shifting the window one character at a time until the end of the text example.\n",
        "\n",
        "* Conversion to Integer Indices: Retrieve integer indices corresponding to characters in both data features and target values based on the previously constructed vocabulary. This step transforms characters into their corresponding numerical representations.\n",
        "\n",
        "* Embeddings Assignment: Each unique integer index, representing a specific character in the data features, is associated with a real-valued vector known as an embedding. These embeddings provide a continuous representation of characters, facilitating numerical computation within the neural network. This is optional"
      ],
      "metadata": {
        "id": "P-BCYZblqhzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loading"
      ],
      "metadata": {
        "id": "G6S6h2ucqh15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, valid_dataset, test_dataset = datasets.PennTreebank()"
      ],
      "metadata": {
        "id": "SZdFbkIK02p2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataset.shuffle()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1eY6iOdon2zK",
        "outputId": "4fdd32d6-9a19-4562-d45c-72d22555bd81"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'instead new york city police seized the stolen goods and mr. <unk> avoided jail'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def info(x):\n",
        "  return len(x)\n",
        "\n",
        "elem_ls = list(train_dataset.map(info))"
      ],
      "metadata": {
        "id": "rLbCiNQgqjCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dccc9c33-8efc-4955-9c13-4fed86ba7060"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(elem_ls)) # total 42k elements\n",
        "print(max(elem_ls)) # max length of each element\n",
        "print(min(elem_ls)) # min length of each element"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lijfysg4qjGt",
        "outputId": "2bb95621-0ae9-49e4-a4a2-2319e48c8a97"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42068\n",
            "518\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We construct a vocabulary of unique characters using build_vocab_from_iterator() from torchtext's 'vocab' sub-module. Our custom function build_vocabulary() serves as an iterator, looping through datasets and examples to yield character lists. Special handling ensures the '<unk>' token, representing unknown characters, is counted as a single token rather than individual characters.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cn-JNFvXqicC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(datasets):\n",
        "  for dataset in datasets:\n",
        "    for text in dataset:\n",
        "\n",
        "      if \"unk\" in text:\n",
        "        texts = text.split(\"<unk>\")\n",
        "        total = list(texts[0].lower())\n",
        "        for t in texts[1:]:\n",
        "            total.extend([\"<unk>\", ] + list(t.lower()))\n",
        "        yield total\n",
        "\n",
        "      else:\n",
        "        yield list(text.lower())"
      ],
      "metadata": {
        "id": "jXD9aSWCqigr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, valid_dataset, test_dataset]), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "dlARGGorqik4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akAGACCbqipt",
        "outputId": "f5b1459a-a711-48ab-d8e2-a1bdc19c12c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab.get_itos()) # character level tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFE7iwcEqitu",
        "outputId": "617119c4-6640-4d70-dd61-ce23325f6e71"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', ' ', 'e', 't', 'a', 'n', 'o', 'i', 's', 'r', 'h', 'l', 'd', 'c', 'u', 'm', 'f', 'p', 'g', 'y', 'b', 'w', 'v', 'k', '.', \"'\", 'x', 'j', '$', '-', 'q', 'z', '&', '0', '1', '9', '3', '#', '2', '8', '5', '\\\\', '7', '6', '/', '4', '*']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab.get_stoi()) # dictionary mapping token to indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U5HYqACqiyY",
        "outputId": "89c1a60c-a5a1-4b21-a8fa-e5f03e49bd44"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'4': 45, '/': 44, '7': 42, '8': 39, '2': 38, '#': 37, '9': 35, '1': 34, 'z': 31, 'q': 30, '-': 29, '6': 43, '3': 36, 'r': 9, 's': 8, 'd': 12, 'k': 23, 'n': 5, 'h': 10, '*': 46, 'u': 14, '0': 33, 'p': 17, 't': 3, 'i': 7, '\\\\': 41, '5': 40, 'a': 4, 'e': 2, 'j': 27, '&': 32, 'v': 22, 'o': 6, '<unk>': 0, '.': 24, 'c': 13, 'm': 15, 'f': 16, 'l': 11, 'g': 18, 'y': 19, 'b': 20, 'w': 21, ' ': 1, 'x': 26, \"'\": 25, '$': 28}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the sequential data for training with sliding window approach and window size of 10 characters"
      ],
      "metadata": {
        "id": "iG4thutWqi1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 35\n",
        "train_records_max = 5000\n",
        "X_train_full, y_train_full = [], []\n",
        "X_val_full , y_val_full = [], []"
      ],
      "metadata": {
        "id": "z25zZbIyqi5s"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train data prep\n",
        "for idex, text in enumerate(train_dataset):\n",
        "  print(text)\n",
        "  print(\"\\n\")\n",
        "  for i in range(len(text) - seq_len):\n",
        "    inp_rec = list(text[i:i+seq_len].lower())\n",
        "    op_rec = text[i+seq_len].lower()\n",
        "\n",
        "    if len(op_rec) == 0:\n",
        "      break\n",
        "\n",
        "    X_train_full.append(vocab(inp_rec))\n",
        "    y_train_full.append(vocab[op_rec])\n",
        "\n",
        "  if idex > train_records_max:\n",
        "    break"
      ],
      "metadata": {
        "id": "3CnjBJ8-qjKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train_full))\n",
        "print(len(y_train_full))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7KNmOsFvnpW",
        "outputId": "99b0fd49-8501-404b-9a8b-b038bd7aeb71"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "423585\n",
            "423585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation dataset prep\n",
        "for idex, text in enumerate(valid_dataset):\n",
        "  print(text)\n",
        "  print(\"\\n\")\n",
        "  for i in range(len(text) - seq_len):\n",
        "    inp_rec = list(text[i:i+seq_len].lower())\n",
        "    op_rec = text[i+seq_len].lower()\n",
        "\n",
        "    if len(op_rec) == 0:\n",
        "      break\n",
        "\n",
        "    X_val_full.append(vocab(inp_rec))\n",
        "    y_val_full.append(vocab[op_rec])"
      ],
      "metadata": {
        "id": "McJdS1AP7ni8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_val_full))\n",
        "print(len(y_val_full))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95RQWNLC75ac",
        "outputId": "ba629277-4025-4a0f-a4af-a1476e556b9d"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "273982\n",
            "273982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train_full, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train_full)\n",
        "print(f\"The shape of X_train is {X_train.shape}\") # n records with k elements in each\n",
        "print(f\"The shape of Y_train is {y_train.shape}\") # n records with 1 element in each"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggzw-zK4tqFA",
        "outputId": "4ae6b9f9-35cf-4d85-cbd3-73196aad6fc2"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of X_train is torch.Size([423585, 35])\n",
            "The shape of Y_train is torch.Size([423585])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = torch.tensor(X_val_full, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val_full)\n",
        "print(f\"The shape of X_train is {X_val.shape}\") # n records with k elements in each\n",
        "print(f\"The shape of Y_train is {y_val.shape}\") # n records with 1 element in each"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB9gMxk37xbj",
        "outputId": "a6cdfbff-5b08-4746-c827-8a7a671241e2"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of X_train is torch.Size([273982, 35])\n",
            "The shape of Y_train is torch.Size([273982])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not embedding:\n",
        "  X_train = X_train.unsqueeze(dim=-1)\n",
        "  X_val = X_val.unsqueeze(dim=-1)"
      ],
      "metadata": {
        "id": "9AL8PPkHtqqk"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader part"
      ],
      "metadata": {
        "id": "jKlO6RJdtqvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(vectorized_train_dataset, batch_size=1024, shuffle=False)\n",
        "\n",
        "vectorized_valid_dataset = TensorDataset(X_val, y_val)\n",
        "valid_loader = DataLoader(vectorized_valid_dataset, batch_size=1024, shuffle=False)"
      ],
      "metadata": {
        "id": "YzJDN7vNtqyU"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in train_loader:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn2Dh5n6tq2C",
        "outputId": "534de85a-bc88-43f4-b2de-d5d5b6512292"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024, 35, 1])\n",
            "torch.Size([1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling Building using Character based RNN"
      ],
      "metadata": {
        "id": "F2J-y8qGtq5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The network includes 2 LSTM layers with an output size of 256 each, followed by a linear layer. Stacking these LSTM layers enhances sequence learning. The output of the second LSTM layer feeds into the linear layer, whose output units match the vocabulary size"
      ],
      "metadata": {
        "id": "YKAcceut0T4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 256\n",
        "n_layers=2\n",
        "\n",
        "class LSTMTextGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMTextGenerator, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, len(vocab))\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "      # init weights\n",
        "      hidden = torch.randn(n_layers, len(X_batch), hidden_dim).to(device)\n",
        "      carry = torch.randn(n_layers, len(X_batch), hidden_dim).to(device)\n",
        "\n",
        "      output, (hidden, carry) = self.lstm(X_batch, (hidden, carry))\n",
        "      return self.linear(output[:,-1])"
      ],
      "metadata": {
        "id": "PNEtpfZb0T8p"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_generator_lstm = LSTMTextGenerator().to(device)"
      ],
      "metadata": {
        "id": "Y6ObvTGk0UAU"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in text_generator_lstm.children():\n",
        "    print(\"Layer : {}\".format(layer))\n",
        "    print(\"Parameters : \")\n",
        "    for param in layer.parameters():\n",
        "        print(param.shape)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pxg1_m20UD0",
        "outputId": "6751f4df-4d39-448d-bcc8-c4af3d3039c0"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer : LSTM(1, 256, num_layers=2, batch_first=True)\n",
            "Parameters : \n",
            "torch.Size([1024, 1])\n",
            "torch.Size([1024, 256])\n",
            "torch.Size([1024])\n",
            "torch.Size([1024])\n",
            "torch.Size([1024, 256])\n",
            "torch.Size([1024, 256])\n",
            "torch.Size([1024])\n",
            "torch.Size([1024])\n",
            "\n",
            "\n",
            "Layer : Linear(in_features=256, out_features=47, bias=True)\n",
            "Parameters : \n",
            "torch.Size([47, 256])\n",
            "torch.Size([47])\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_module(model:torch.nn.Module,\n",
        "                 device:torch.device,\n",
        "                 train_dataloader:torch.utils.data.DataLoader ,\n",
        "                 optimizer:torch.optim.Optimizer,\n",
        "                 criterion:torch.nn.Module,\n",
        "                 metric,\n",
        "                 train_losses:list,\n",
        "                 train_metrics:list):\n",
        "\n",
        "  # setting model to train mode\n",
        "  model.train()\n",
        "  pbar = tqdm(train_dataloader)\n",
        "\n",
        "  # batch metrics\n",
        "  train_loss = 0\n",
        "  processed_batch = 0\n",
        "\n",
        "  for idx, (data,label) in enumerate(pbar):\n",
        "    # setting up device\n",
        "    data = data.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    # forward pass output\n",
        "    preds = model(data)\n",
        "\n",
        "    # calc loss\n",
        "    loss = criterion(preds, label)\n",
        "    train_loss += loss.item()\n",
        "    # print(f\"training loss for batch {idx} is {loss}\")\n",
        "\n",
        "    # backpropagation\n",
        "    optimizer.zero_grad() # flush out  existing grads\n",
        "    loss.backward() # back prop of weights wrt loss\n",
        "    optimizer.step() # optimizer step -> minima\n",
        "\n",
        "    #updating batch count\n",
        "    processed_batch += 1\n",
        "\n",
        "    pbar.set_description(f\"Avg Train Loss: {train_loss/processed_batch}\")\n",
        "\n",
        "  # updating epoch metrics\n",
        "  train_losses.append(train_loss/processed_batch)\n",
        "\n",
        "  return train_losses\n"
      ],
      "metadata": {
        "id": "YT0arD4T0UHK"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_module(model:torch.nn.Module,\n",
        "                device:torch.device,\n",
        "                test_dataloader:torch.utils.data.DataLoader,\n",
        "                criterion:torch.nn.Module,\n",
        "                metric,\n",
        "                test_losses,\n",
        "                test_metrics):\n",
        "  # setting model to eval mode\n",
        "  model.eval()\n",
        "  pbar = tqdm(test_dataloader)\n",
        "\n",
        "  # batch metrics\n",
        "  test_loss = 0\n",
        "  processed_batch = 0\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for idx, (data,label) in enumerate(pbar):\n",
        "      data , label = data.to(device), label.to(device)\n",
        "      # predictions\n",
        "      preds = model(data)\n",
        "      # print(preds.shape)\n",
        "      # print(label.shape)\n",
        "\n",
        "      #loss calc\n",
        "      loss = criterion(preds, label)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      #updating batch count\n",
        "      processed_batch += 1\n",
        "\n",
        "      pbar.set_description(f\"Avg Test Loss: {test_loss/processed_batch}\")\n",
        "\n",
        "    # updating epoch metrics\n",
        "    test_losses.append(test_loss/processed_batch)\n",
        "\n",
        "  return test_losses"
      ],
      "metadata": {
        "id": "fy24QjwH0UKu"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(text_generator_lstm.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "JOx7aMhF0UOq"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Place holders----\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(0,epochs):\n",
        "  print(f'Epoch {epoch}')\n",
        "  train_losses = train_module(text_generator_lstm, device, train_loader, optimizer, criterion, None, train_losses, None)\n",
        "  test_losses = test_module(text_generator_lstm, device, valid_loader, criterion, None, test_losses, None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JnOZBmf0UTT",
        "outputId": "b74777c5-7d38-4f8d-b7f4-f55b5511ca32"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 3.0016534409085334: 100%|██████████| 414/414 [00:33<00:00, 12.31it/s]\n",
            "Avg Test Loss: 2.8839129342961667: 100%|██████████| 268/268 [00:11<00:00, 23.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 2.848298272072981: 100%|██████████| 414/414 [00:34<00:00, 11.91it/s]\n",
            "Avg Test Loss: 2.800626163162402: 100%|██████████| 268/268 [00:10<00:00, 24.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 2.7244479212783963: 100%|██████████| 414/414 [00:34<00:00, 11.86it/s]\n",
            "Avg Test Loss: 2.659426087763772: 100%|██████████| 268/268 [00:10<00:00, 24.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 2.605129315657316: 100%|██████████| 414/414 [00:35<00:00, 11.79it/s]\n",
            "Avg Test Loss: 2.571557663269897: 100%|██████████| 268/268 [00:10<00:00, 25.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 2.5361479572627856: 100%|██████████| 414/414 [00:35<00:00, 11.71it/s]\n",
            "Avg Test Loss: 2.512654185295105: 100%|██████████| 268/268 [00:11<00:00, 23.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 2.484039280725562: 100%|██████████| 414/414 [00:35<00:00, 11.80it/s]\n",
            "Avg Test Loss: 2.463906225873463: 100%|██████████| 268/268 [00:11<00:00, 23.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 2.4395451344153734: 100%|██████████| 414/414 [00:34<00:00, 11.84it/s]\n",
            "Avg Test Loss: 2.421219460999788: 100%|██████████| 268/268 [00:11<00:00, 23.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 2.4010436287248766: 100%|██████████| 414/414 [00:35<00:00, 11.78it/s]\n",
            "Avg Test Loss: 2.385412451046616: 100%|██████████| 268/268 [00:11<00:00, 23.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 2.367971516461764: 100%|██████████| 414/414 [00:35<00:00, 11.76it/s]\n",
            "Avg Test Loss: 2.3539513789895756: 100%|██████████| 268/268 [00:10<00:00, 24.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 2.3384690310644065: 100%|██████████| 414/414 [00:35<00:00, 11.82it/s]\n",
            "Avg Test Loss: 2.326124023590515: 100%|██████████| 268/268 [00:11<00:00, 23.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7min 28s, sys: 4.09 s, total: 7min 32s\n",
            "Wall time: 7min 40s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "The logic starts with the initial randomly selected sequence and makes the next character prediction. It then removes the first character from the sequence and adds a newly predicted character at the end. Then, it makes another prediction and the process repeats for 100 characters."
      ],
      "metadata": {
        "id": "pGhUMepStq9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randint(0, len(X_train))\n",
        "pattern = X_train[idx].numpy().astype(int).flatten().tolist() # list of tokens matched with torch text\n",
        "print(\"Initial Pattern : {}\".format(\"\".join(vocab.lookup_tokens(pattern))))"
      ],
      "metadata": {
        "id": "CYzGR6U--FM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74b3737-28f7-4ecc-969a-1264744167ad"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Pattern : enging a university faculty member \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = []\n",
        "# genererate 100 characters and also offsetting one character after every iterations to maintain the seq length\n",
        "for i in range(100):\n",
        "  batch = torch.tensor(pattern, dtype=torch.float32).reshape(1, seq_len, 1).to(device)\n",
        "  model_op = text_generator_lstm(batch)\n",
        "  # print(model_op.shape) # 47 is the vocab size\n",
        "  predicted_index = model_op.argmax(dim=-1).squeeze().cpu().item()\n",
        "  generated_text.append(predicted_index) ## Add token index to result\n",
        "  pattern.append(predicted_index) ## Add token index to original pattern\n",
        "  pattern = pattern[1:] ## Resize pattern to bring again to seq_length l\n"
      ],
      "metadata": {
        "id": "pitTDIYYLSlW"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text : {}\".format(\"\".join(vocab.lookup_tokens(generated_text))))"
      ],
      "metadata": {
        "id": "hc_-69w9-Feu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2f32fe-5eb6-4d77-8011-a14056df0468"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text : a n a n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is producing some random text but english text but in repeations\n",
        "and even after mutiple training yet it produces similar random text"
      ],
      "metadata": {
        "id": "2o33Z-LiLYd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building using self trained embeddings\n",
        "\n",
        "We have used character-based approach for our case which means that our network takes a list of characters as input and returns the next character that it thinks should come next. We can also design models that take a list of words as input and predicts the next word. For encoding text data, we have used character embeddings approach which assigns a real-valued vector to each token (character)"
      ],
      "metadata": {
        "id": "28gx9rd2Ld1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network Architecture:\n",
        "\n",
        "* Embedding Layer: 100 embedding length, input (batch_size, seq_length), output (batch_size, seq_length, 100).\n",
        "* LSTM Layer 1 & 2: 256 hidden dimensions, input (batch_size, seq_length, embed_len), output (batch_size, seq_length, 256).\n",
        "* Linear Layer: Output units match vocabulary length, input (batch_size, seq_length, 256), output (batch_size, vocab_len).\n",
        "* Embedding Layer:\n",
        "\n",
        "  Utilizes Embedding() constructor with vocab length and 100 embedding length.\\\n",
        "  Transforms input shape to (batch_size, seq_length, embed_len).\n",
        "\n",
        "* LSTM Layers:\n",
        "\n",
        "  LSTM Layer 1 processes embedding output with 256 hidden dimensions.\\\n",
        "  LSTM Layer 2 processes LSTM 1 output with 256 hidden dimensions.\n",
        "\n",
        "* Linear Layer:\\\n",
        "  Transforms LSTM 2 output to (batch_size, vocab_len), representing predictions.\n",
        "\n",
        "* Initialization & Verification:\\\n",
        "  Initialized network and examined weights/biases.\\\n",
        "  Conducted forward pass with sample data for validation."
      ],
      "metadata": {
        "id": "NMpwX6f7NuGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train_full, dtype=torch.int64)\n",
        "y_train = torch.tensor(y_train_full)\n",
        "print(f\"The shape of X_train is {X_train.shape}\") # n records with k elements in each\n",
        "print(f\"The shape of Y_train is {y_train.shape}\") # n records with 1 element in each"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ec78bf-8331-4408-dbe7-c83a004f1e6e",
        "id": "Z9NIEF-rVv6Q"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of X_train is torch.Size([423585, 35])\n",
            "The shape of Y_train is torch.Size([423585])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = torch.tensor(X_val_full, dtype=torch.int64)\n",
        "y_val = torch.tensor(y_val_full)\n",
        "print(f\"The shape of X_train is {X_val.shape}\") # n records with k elements in each\n",
        "print(f\"The shape of Y_train is {y_val.shape}\") # n records with 1 element in each"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da98ee43-0573-4ee6-98d5-e0f456e83f50",
        "id": "OHum0m0pVv6R"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of X_train is torch.Size([273982, 35])\n",
            "The shape of Y_train is torch.Size([273982])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new data loader\n",
        "vectorized_train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(vectorized_train_dataset, batch_size=1024, shuffle=False)\n",
        "\n",
        "vectorized_valid_dataset = TensorDataset(X_val, y_val)\n",
        "valid_loader = DataLoader(vectorized_valid_dataset, batch_size=1024, shuffle=False)"
      ],
      "metadata": {
        "id": "MYhvcNR6Wcjh"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_len = 100\n",
        "hidden_dim = 256\n",
        "n_layers=2\n",
        "\n",
        "class LSTMTextGenerator_Embed(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMTextGenerator_Embed, self).__init__()\n",
        "        self.word_embedding = nn.Embedding(num_embeddings= 47, embedding_dim=embed_len)\n",
        "        self.lstm = nn.LSTM(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, len(vocab))\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.word_embedding(X_batch)\n",
        "\n",
        "        hidden, carry = torch.randn(n_layers, len(X_batch), hidden_dim).to(device), torch.randn(n_layers, len(X_batch), hidden_dim).to(device)\n",
        "        output, (hidden, carry) = self.lstm(embeddings, (hidden, carry))\n",
        "        return self.linear(output[:,-1])"
      ],
      "metadata": {
        "id": "NYKvVyUONuKj"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_generator_lstm_embd = LSTMTextGenerator_Embed().to(device)"
      ],
      "metadata": {
        "id": "s-8YntL0NuOh"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(text_generator_lstm_embd.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "2INLFK8CNuSh"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Place holders----\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(0,epochs):\n",
        "  print(f'Epoch {epoch}')\n",
        "  train_losses = train_module(text_generator_lstm_embd, device, train_loader, optimizer, criterion, None, train_losses, None)\n",
        "  test_losses = test_module(text_generator_lstm_embd, device, valid_loader, criterion, None, test_losses, None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEixgUjgNuWh",
        "outputId": "0a771dbd-34e9-4098-890c-1f38f8d12742"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 2.8572771076994816: 100%|██████████| 414/414 [00:36<00:00, 11.21it/s]\n",
            "Avg Test Loss: 2.4506678554549146: 100%|██████████| 268/268 [00:11<00:00, 23.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 2.282755870174095: 100%|██████████| 414/414 [00:42<00:00,  9.77it/s]\n",
            "Avg Test Loss: 2.171659545667136: 100%|██████████| 268/268 [00:14<00:00, 17.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 2.100640987716435: 100%|██████████| 414/414 [00:38<00:00, 10.73it/s]\n",
            "Avg Test Loss: 2.045561326083852: 100%|██████████| 268/268 [00:12<00:00, 21.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 1.9968238844387773: 100%|██████████| 414/414 [00:37<00:00, 11.12it/s]\n",
            "Avg Test Loss: 1.9571161310174572: 100%|██████████| 268/268 [00:11<00:00, 22.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 1.9185841598948419: 100%|██████████| 414/414 [00:38<00:00, 10.84it/s]\n",
            "Avg Test Loss: 1.887179025963171: 100%|██████████| 268/268 [00:11<00:00, 23.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 1.8544545648754507: 100%|██████████| 414/414 [00:38<00:00, 10.82it/s]\n",
            "Avg Test Loss: 1.829539271432962: 100%|██████████| 268/268 [00:11<00:00, 23.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 1.7997887341872505: 100%|██████████| 414/414 [00:38<00:00, 10.80it/s]\n",
            "Avg Test Loss: 1.7806454825757154: 100%|██████████| 268/268 [00:11<00:00, 23.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 1.7519318484453763: 100%|██████████| 414/414 [00:38<00:00, 10.70it/s]\n",
            "Avg Test Loss: 1.7381872321242717: 100%|██████████| 268/268 [00:11<00:00, 23.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 1.7088922089424685: 100%|██████████| 414/414 [00:38<00:00, 10.88it/s]\n",
            "Avg Test Loss: 1.7004961682789361: 100%|██████████| 268/268 [00:11<00:00, 23.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avg Train Loss: 1.6701050587898292: 100%|██████████| 414/414 [00:37<00:00, 10.92it/s]\n",
            "Avg Test Loss: 1.6672258777404898: 100%|██████████| 268/268 [00:11<00:00, 23.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8min 1s, sys: 4.13 s, total: 8min 5s\n",
            "Wall time: 8min 23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randint(0, len(X_train))\n",
        "pattern = X_train[idx].numpy().astype(int).flatten().tolist() # list of tokens matched with torch text\n",
        "print(\"Initial Pattern : {}\".format(\"\".join(vocab.lookup_tokens(pattern))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dba7bd9-2084-4ba4-c305-5c3cd55bae21",
        "id": "oe87ZBJWzvtZ"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Pattern : lped much by the announcement trade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = []\n",
        "# genererate 100 characters and also offsetting one character after every iterations to maintain the seq length\n",
        "for i in range(100):\n",
        "  batch = torch.tensor(pattern, dtype=torch.int64).reshape(1, seq_len).to(device)\n",
        "  model_op = text_generator_lstm_embd(batch)\n",
        "  # print(model_op.shape) # 47 is the vocab size\n",
        "  predicted_index = model_op.argmax(dim=-1).squeeze().cpu().item()\n",
        "  generated_text.append(predicted_index) ## Add token index to result\n",
        "  pattern.append(predicted_index) ## Add token index to original pattern\n",
        "  pattern = pattern[1:] ## Resize pattern to bring again to seq_length l\n"
      ],
      "metadata": {
        "id": "2dbyikjOzvtk"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text : {}\".format(\"\".join(vocab.lookup_tokens(generated_text))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0807c09-9d45-4548-b784-2e10a2c9eb65",
        "id": "LtlpsRvNzvtk"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text : r and the securition and the securition and the securition and the securition and the securition and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CU-ZlP40NuZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4YnoN7XiNuea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pgm-51c0Nuhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "blBzGzDzNuly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3vdpP8VNupd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_WYHanMhNutf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}